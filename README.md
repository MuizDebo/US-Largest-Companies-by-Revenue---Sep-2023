# US Largest Companies by Revenue Sep 2023 (Analysis Brief)
## Data Collection
In this code, I embark on a web scraping journey to retrieve valuable information from a specific webpage. The destination is a Wikipedia page titled "List of largest companies in the United States by revenue." Let's dive into the steps taken to accomplish this task.

First, I start by defining the URL of the webpage I intend to scrape. This URL, 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue,' serves as our digital gateway to a treasure trove of data.

With the URL in hand, I proceeded to make an HTTP request using the trusty 'requests' library. This request allows me to fetch the webpage's content, bringing it into my Jupyter Notebook for further exploration.

Now, it's time to unveil the true magic – creating a BeautifulSoup object. I use the 'html.parser' to craft this object to help me decipher and navigate the intricate HTML content of the webpage.

As I delve deeper into the HTML structure, I hone in on a specific table that holds the data I seek. This table is the second one on the page, and I select it using 'soup.find_all('table')[1].' 

Next, I set my sights on the column titles; those headers that give structure to the table. I extracted them by locating all 'th' elements within the table, ensuring that any whitespace is stripped away.

The true essence of this code lies in the creation of a new Pandas DataFrame, which I named 'new_table.' Its columns are named after the extracted table titles, ready to be filled with valuable insights.

Then, I embarked on the task of extracting data from each row of the table. With each iteration, I find all 'td' elements within the row, extracting and formatting their text to create a list of individual cell values. 

With these individual row data in hand, I added them to my 'new_table' DataFrame, row by row, bringing the table to life with each iteration.

Finally, I saved the populated DataFrame as a CSV file and ensured that the index is left behind – only the valuable data finds its way into the CSV file.

## Analysis Section
In my data analysis, I delved into various aspects of my dataset, which includes information about companies. I initiated my exploration by conducting descriptive statistics on the 'Revenue(USD)' column. The data revealed that the average revenue stands at $117566.46, with a median of $80824.5. The standard deviation, an indicator of data dispersion, is approximately $100915.11. My dataset's revenue spans from a minimum of $44200 to a maximum of $611289.

Moving on to industry analysis, I grouped the data by industry and unveiled a range of insights specific to each sector. My visualization of revenue distribution across industries exposed 'retail' companies have the highest grownth in revenue, followed by 'retail cloud computing' companies while the least performing sectors are the 'apparel' and 'laboratory instruments' companies in my dataset.

To gain insights into the geographical distribution of companies, I examined headquarters locations. This analysis illuminated that 'New York', 'Texas' and 'California' are where most companies are headquartered, providing valuable geographic context to my dataset.

Revenue analysis was a key focal point. A scatter plot of revenue versus rank showcased a positive correlation about the distribution of revenue within the dataset. Moreover, I calculated the total revenue generated by all companies, with the total amounting to $11756646.

Revenue growth was another critical metric under scrutiny. I uncovered from my analysis of revenue growth distributions and identified companies with the highest and lowest growth rates, shedding light on the dynamism within my dataset. My analysis showed the the top growing industries in terms of revenue were 'infotech', 'petroleum and logistics' and 'airline' respectively.

Employee analysis offered insights into workforce dynamics. The distribution of employee counts was visualized. Additionally, I identified companies with the most employees and those with the fewest employees, offering valuable perspectives on workforce size and composition. My analysis showed the the top companies in terms of employees were 'Walmart', 'Amazon' and 'United States Postal Service' respectively.

Rank analysis allowed me to explore how rank relates to financial performance. I calculated the correlation between rank and revenue, which resulted in a correlation coefficient of ~-0.78, indicating the low strength and direction of this association.

Finally, my multi-variable analysis, in the form of a scatter plot comparing revenue to employee counts, highlighted skewness to the right about the interplay between these two vital aspects of company data.

For time series analysis and other specific insights, please refer to the respective sections of my analysis for a comprehensive understanding of my dataset's intricacies. Overall, this analysis provides a comprehensive view of the companies within my dataset, offering valuable insights into their financial performance, industry dynamics, geographic distribution, and more.
